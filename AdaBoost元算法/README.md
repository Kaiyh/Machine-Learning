## 基于数据集多重抽样的分类器

1、bagging方法

从原始数据集选择S次后得到S个新数据集的一种技术。在S个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了S个分类器，选择分类器投票结果中最多的类别作为最后的分类结果。

有一些更先进的bagging方法，比如 __随机森林（random forest）__。

2、boosting方法

通过集中关注被已有分类器错分的那些数据来获取新的分类器，boosting分类的结果是基于所有分类器的加权求和的结果，其中每个分类器的权重并不相等，代表的是其对应分类器在上一轮迭代中的成功度。

boosting方法有多个版本，我们只关注其中一个最流行的版本 __AdaBoost__。

## AdaBoost算法

AdaBoost(Adaptive Boosting)是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器，即弱分类器，然后把这些弱分类器集合起来，构造一个更强的最终分类器。

AdaBoost算法的运行过程如下：

1. 对训练数据中的每个样本赋予一个权重，这些权重构成了 __权重向量D__。一开始，这些权重都初始化成相等值。首先在训练数据上训练出一个 __弱分类器__ 并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。

2. 在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个 __权重值alpha__，这些alpha值是基于每个弱分类器的 __错误率e__ 进行计算的。

3. 计算出alpha值之后，可以对 __权重向量D__ 进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。

4. 在计算出权重向量D之后，AdaBoost又开始进入下一轮的 __迭代__。AdaBoost算法会不断地重复训练和调整权重的过程，直到训练错误率为0或者弱分类器的数目达到用户的指定值为止。

其中，

错误率e定义为：`e = 未正确分类的样本数目 / 所有样本数目`

alpha的计算公式为：`a = 0.5 * ln((1-e) / e)`

对向量D进行更新的规则：  
1）如果某个样本被正确分类，`D = (D * exp(-a)) / sum(D)`   
2）如果某个样本被错分，`D = (D * exp(a)) / sum(D)`

## 基于单层决策树构建弱分类器

我们使用单层决策树作为 __弱分类器__。单层决策树是一种简单的决策树，它仅由单个特征来做决策，这棵树只有一次分裂过程，因此它实际上就是一个树桩。

    将最小错误率minError设为正无穷
    对数据集中的每一个特征（第一次循环）：
        对每个步长（第二次循环）：
            对每个不等号（第三层循环）：
                建立一棵单层决策树并利用加权数据集对它进行测试
                如果错误率低于minError，则将当前单层决策树设为最佳单层决策树
    返回最佳单层决策树

代码详见 [adaboost.py -> buildStump()](adaboost.py)

## AdaBoost算法的实现

1、基于单层决策树的AdaBoost训练过程

    对每次迭代：
        利用buildStump()函数找到最佳的单层决策树
        将最佳单层决策树加入到单层决策树数组
        计算alpha
        计算新的权重向量D
        更新累计类别估计值
        如果错误率等于0.0，退出循环

代码详见 [adaboost.py -> adaBoostTrainDS()](adaboost.py)
    
2、基于AdaBoost的分类

    对每个弱分类器的结果加权求和
    返回最终的分类结果

代码详见 [adaboost.py -> adaClassify()](adaboost.py)

## 算法应用

利用AdaBoost预测患疝病马的死亡率

代码详见 [adaboost.py](adaboost.py)