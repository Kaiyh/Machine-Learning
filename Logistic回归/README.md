## Logistic回归

> 回归：假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合的过程就成为 _回归_

利用Logistic回归进行分类的主要思想是：__根据现有数据对分类边界线建立回归公式，以此进行分类__

## Sigmoid函数

为了实现分类，可以考虑利用分段函数，比如常见的二值函数：在两个类的情况下，函数输出0或1；该函数称为海维赛德阶跃函数(Heaviside step function)，或单位阶跃函数。

为了避免瞬间跳跃，我们使用Sigmoid函数：`D(z) = 1 / (1 + exp(-z))`

性质：当z为0时，Sigmoid函数值为0.5；随着z的增大，对应的函数值D(z)将逼近于1；而随着x的减小，函数值D(z)将逼近于0；如果横坐标刻度足够大，Sigmoid函数看起来很像一个阶跃函数。

## 利用Logistic回归和Sigmoid函数进行分类

Logistic回归可以看成是一种概率估计：为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0~1之间的数值，任何大于0.5的数据被分入1类，小于0.5即被归入0类。

因此，利用Logistic回归进行分类的步骤总结如下：

1. 根据训练集寻找 _最佳回归系数_
2. 将最佳回归系数分别与对应的样本特征值进行相乘并求和
3. 将计算结果作为Sigmoid函数的输入参数，根据返回值进行分类

## 基于最优化方法来确定最佳回归系数

寻找Logistic回归方法中的最佳回归系数，是算法最为关键的一步。

可形式化的表示为：`z = w0x0 + w1x1 + w2x2 + ... + wnxn`

其中，`z`表示计算结果即Sigmoid函数的输入，`x`表示数据样本每一个特征属性的值，`w`表示与特征对应的回归系数

_为了更具一般性，我们可以将这里的x看做多维空间下中每一个坐标轴变量（对应样本点的每一个特征），w看做每一维的参数（对应特征的参数），那么z就是一条多维空间直线，这条直线可以将所有的数据点分割成不同的部分_

我们采用 _最优化方法_：梯度上升法 来确定回归系数W (`W = w0w1w2...wn`)

1. 梯度上升法

梯度上升法的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。

移动方向：梯度算子总是指向函数值增长最快的方向；移动量的大小：称为步长，可以根据实际数据赋初值，并进行动态修改

梯度上升算法的迭代公式：`w := w + a * Gf(w)` (a表示步长，G表示函数f在该点的梯度)

2. 随机梯度上升

相比于梯度上升法，改进的方法是：一次仅用一个样本点来更新回归系数，称为 _随机梯度上升法_。由于可以在新样本到来时对分类器进行增量式更新，因而它是一个在线学习算法。

进一步改进随机梯度上升法：1、在每次迭代的时候调整步长，步长会随着迭代次数不断减小（趋于0）  2、通过随机选取样本来更新回归系数

代码详见：[logRegres.py -> stocGradAscent1()](logRegres.py)

## 算法应用

从疝气病症预测病马的死亡率

代码详见：[logRegres.py](logRegres.py)